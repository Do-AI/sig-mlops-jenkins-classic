{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classifier\n",
    "\n",
    "Implementation of a news classifier model using Scikit Learn's Naive Bayes implementation.\n",
    "Since this model is implemented using Scikit Learn, we can deploy it using [one of Seldon's pre-built re-usable server](https://docs.seldon.io/projects/seldon-core/en/latest/servers/sklearn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First we will train a machine learning model, which will help us classify news across multiple categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies \n",
    "\n",
    "We will need the following dependencies in order to run the Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# You need the right versions for your model server:\n",
      "# Model servers: https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html\n",
      "\n",
      "# For SKLearn you need a pickle and the following:\n",
      "scikit-learn==0.20.3 # See https://docs.seldon.io/projects/seldon-core/en/latest/servers/sklearn.html\n",
      "joblib==0.13.2\n",
      "\n",
      "# For XGBoost you need v 0.82 and an xgboost export (not a pickle)\n",
      "#xgboost==0.82\n",
      "\n",
      "# For MLFlow you need the following, and a link to the built model:\n",
      "#mlflow==1.1.0\n",
      "#pandas==0.25\n",
      "\n",
      "# For tensorflow, any models supported by tensorflow serving (less than v2.0)\n",
      "\n",
      "# For testing\n",
      "pytest==5.1.1\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now install the dependencies using the make command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: VERSION: No such file or directory\n",
      "Makefile:10: warning: overriding recipe for target 'make'\n",
      "Makefile:7: warning: ignoring old recipe for target 'make'\n",
      "Makefile:14: warning: overriding recipe for target 'make'\n",
      "Makefile:10: warning: ignoring old recipe for target 'make'\n",
      "pip install -r src/requirements.txt\n",
      "Collecting scikit-learn==0.20.3 (from -r src/requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/82/c0de5839d613b82bddd088599ac0bbfbbbcbd8ca470680658352d2c435bd/scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4MB 3.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib==0.13.2 (from -r src/requirements.txt (line 6))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 44.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest==5.1.1 (from -r src/requirements.txt (line 18))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/3b/5652e27e048ae086f79ce9c4ce8a2da6bad1e9590788e5768aafc6f375ef/pytest-5.1.1-py3-none-any.whl (223kB)\n",
      "\u001b[K     |████████████████████████████████| 225kB 37.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.13.3 (from scikit-learn==0.20.3->-r src/requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/18/d7c101d5e93b6c78dc206fcdf7bd04c1f8138a7b1a93578158fa3b132b08/scipy-1.3.3-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "\u001b[K     |████████████████████████████████| 25.2MB 11.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /home/agm/.pyenv/versions/3.6.9/lib/python3.6/site-packages (from scikit-learn==0.20.3->-r src/requirements.txt (line 5)) (1.17.4)\n",
      "Collecting py>=1.5.0 (from pytest==5.1.1->-r src/requirements.txt (line 18))\n",
      "  Using cached https://files.pythonhosted.org/packages/76/bc/394ad449851729244a97857ee14d7cba61ddb268dce3db538ba2f2ba1f0f/py-1.8.0-py2.py3-none-any.whl\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest==5.1.1->-r src/requirements.txt (line 18))\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /home/agm/.pyenv/versions/3.6.9/lib/python3.6/site-packages (from pytest==5.1.1->-r src/requirements.txt (line 18)) (0.23)\n",
      "Collecting packaging (from pytest==5.1.1->-r src/requirements.txt (line 18))\n",
      "  Using cached https://files.pythonhosted.org/packages/cf/94/9672c2d4b126e74c4496c6b3c58a8b51d6419267be9e70660ba23374c875/packaging-19.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /home/agm/.pyenv/versions/3.6.9/lib/python3.6/site-packages (from pytest==5.1.1->-r src/requirements.txt (line 18)) (7.2.0)\n",
      "Collecting atomicwrites>=1.0 (from pytest==5.1.1->-r src/requirements.txt (line 18))\n",
      "  Using cached https://files.pythonhosted.org/packages/52/90/6155aa926f43f2b2a22b01be7241be3bfd1ceaf7d0b3267213e8127d41f4/atomicwrites-1.3.0-py2.py3-none-any.whl\n",
      "Collecting wcwidth (from pytest==5.1.1->-r src/requirements.txt (line 18))\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/9f/526a6947247599b084ee5232e4f9190a38f398d7300d866af3ab571a5bfe/wcwidth-0.1.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/agm/.pyenv/versions/3.6.9/lib/python3.6/site-packages (from pytest==5.1.1->-r src/requirements.txt (line 18)) (19.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/agm/.pyenv/versions/3.6.9/lib/python3.6/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.1.1->-r src/requirements.txt (line 18)) (0.6.0)\n",
      "Requirement already satisfied: six in /home/agm/.pyenv/versions/3.6.9/lib/python3.6/site-packages (from packaging->pytest==5.1.1->-r src/requirements.txt (line 18)) (1.12.0)\n",
      "Collecting pyparsing>=2.0.2 (from packaging->pytest==5.1.1->-r src/requirements.txt (line 18))\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/0c/fc2e007d9a992d997f04a80125b0f183da7fb554f1de701bbb70a8e7d479/pyparsing-2.4.5-py2.py3-none-any.whl\n",
      "Installing collected packages: scipy, scikit-learn, joblib, py, pluggy, pyparsing, packaging, atomicwrites, wcwidth, pytest\n",
      "Successfully installed atomicwrites-1.3.0 joblib-0.13.2 packaging-19.2 pluggy-0.13.1 py-1.8.0 pyparsing-2.4.5 pytest-5.1.1 scikit-learn-0.20.3 scipy-1.3.3 wcwidth-0.1.7\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!make install_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the ML data\n",
    "\n",
    "Now that we have all the dependencies we can proceed to download the data.\n",
    "\n",
    "We will download the news stories dataset, and we'll be attempting to classify across the four classes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(\n",
    "    subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(\n",
    "    subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Printing the top 3 newstories\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Now that we've downloaded the data, we can train the ML model using a simple pipeline with basic text pre-processors and a Multiclass naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test single prediction\n",
    "\n",
    "Now that we've trained our model we can use it to predict from un-seen data.\n",
    "\n",
    "We can see below that the model is able to predict the first datapoint in the dataset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTENT:\n",
      "Subject: Re: HELP for Kidney Stones ..............\n",
      "Organization: The Avant-Garde of the Now, Ltd.\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: ucsd.edu\n",
      "\n",
      "As I recall from my bout with kidney stones, there isn't \n",
      "\n",
      "-----------\n",
      "\n",
      "PREDICTED CLASS: comp.graphics\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(f\"CONTENT:{twenty_test.data[idx][35:230]}\\n\\n-----------\\n\")\n",
    "print(f\"PREDICTED CLASS: {categories[twenty_test.target[idx]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print accuracy\n",
    "\n",
    "We can print the accuracy of the model by running the test data and counting the number of correct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print(f\"Accuracy: {np.mean(predicted == twenty_test.target):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Now we want to be able to deploy the model we just trained. This will just be as simple as updated the model binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model\n",
    "\n",
    "First we have to save the trained model in the `src/` folder, which our wrapper will load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src/model.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(text_clf, \"src/model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update your unit test\n",
    "\n",
    "We'll write a very simple unit test that make sure that the model loads and runs as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36munittest\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m mock\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjoblib\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "EXPECTED_RESPONSE = np.array([\u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_model\u001b[39;49;00m(*args, **kwargs):\n",
      "    data = [\u001b[33m\"\u001b[39;49;00m\u001b[33mtext 1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtext 2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    m = joblib.load(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    result = m.predict(data)\n",
      "    \u001b[34massert\u001b[39;49;00m \u001b[36mall\u001b[39;49;00m(result == EXPECTED_RESPONSE)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/test_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: VERSION: No such file or directory\n",
      "Makefile:10: warning: overriding recipe for target 'make'\n",
      "Makefile:7: warning: ignoring old recipe for target 'make'\n",
      "Makefile:14: warning: overriding recipe for target 'make'\n",
      "Makefile:10: warning: ignoring old recipe for target 'make'\n",
      "(cd src && \\\n",
      "\tpytest -s --verbose -W ignore --log-level=INFO 2>&1)\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-5.1.1, py-1.8.0, pluggy-0.13.1 -- /home/agm/.pyenv/versions/3.6.9/bin/python3.6\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/agm/Seldon/sig-mlops-jenkins-classic/models/news_classifier/src\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_model.py::test_model \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m============================== 1 passed in 1.05s ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!make test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Integration Tests\n",
    "\n",
    "We can also now update the integration tests. This is another very simple step, where we'll want to test this model specifically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mseldon_core.seldon_client\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SeldonClient\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mseldon_core.utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m seldon_message_to_json\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m run\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\n",
      "\n",
      "API_AMBASSADOR = \u001b[33m\"\u001b[39;49;00m\u001b[33mlocalhost:8003\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_sklearn_server\u001b[39;49;00m():\n",
      "    data = [\u001b[33m\"\u001b[39;49;00m\u001b[33mFrom: brian@ucsd.edu (Brian Kantor)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mSubject: Re: HELP for Kidney Stones ..............\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mOrganization: The Avant-Garde of the Now, Ltd.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mLines: 12\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mNNTP-Posting-Host: ucsd.edu\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mAs I recall from my bout with kidney stones, there isn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt any\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mmedication that can do anything about them except relieve the pain.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mEither they pass, or they have to be broken up with sound, or they have\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mto be extracted surgically.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mWhen I was in, the X-ray tech happened to mention that she\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33md had kidney\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mstones and children, and the childbirth hurt less.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mDemerol worked, although I nearly got arrested on my way home when I barfed\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mall over the police car parked just outside the ER.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m- Brian\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mFrom: rind@enterprise.bih.harvard.edu (David Rind)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mSubject: Re: Candida(yeast) Bloom, Fact or Fiction\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mOrganization: Beth Israel Hospital, Harvard Medical School, Boston Mass., USA\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mLines: 37\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mNNTP-Posting-Host: enterprise.bih.harvard.edu\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mIn article <1993Apr26.103242.1@vms.ocom.okstate.edu>\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m banschbach@vms.ocom.okstate.edu writes:\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>are in a different class.  The big question seems to be is it reasonable to \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>use them in patients with GI distress or sinus problems that *could* be due \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>to candida blooms following the use of broad-spectrum antibiotics?\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mI guess I\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33mm still not clear on what the term \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mcandida bloom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m means,\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mbut certainly it is well known that thrush (superficial candidal\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33minfections on mucous membranes) can occur after antibiotic use.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mThis has nothing to do with systemic yeast syndrome, the \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mquack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mdiagnosis that has been being discussed.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>found in the sinus mucus membranes than is candida.  Women have been known \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>for a very long time to suffer from candida blooms in the vagina and a \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>women is lucky to find a physician who is willing to treat the cause and \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>not give give her advise to use the OTC anti-fungal creams.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mLucky how?  Since a recent article (randomized controlled trial) of\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33moral yogurt on reducing vaginal candidiasis, I\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33mve mentioned to a \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mnumber of patients with frequent vaginal yeast infections that they\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mcould try eating 6 ounces of yogurt daily.  It turns out most would\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mrather just use anti-fungal creams when they get yeast infections.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>yogurt dangerous).  If this were a standard part of medical practice, as \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>Gordon R. says it is, then the incidence of GI distress and vaginal yeast \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>infections should decline.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mAgain, this just isn\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33mt what the systemic yeast syndrome is about, and\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mhas nothing to do with the quack therapies that were being discussed.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mThere is some evidence that attempts to reinoculate the GI tract with\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mbacteria after antibiotic therapy don\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33mt seem to be very helpful in\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mreducing diarrhea, but I don\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33mt think anyone would view this as a\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mquack therapy.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m-- \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mDavid Rind\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mrind@enterprise.bih.harvard.edu\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    labels = [\u001b[34m2.0\u001b[39;49;00m, \u001b[34m2.0\u001b[39;49;00m]\n",
      "    \n",
      "    sc = SeldonClient(\n",
      "        gateway=\u001b[33m\"\u001b[39;49;00m\u001b[33mambassador\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        gateway_endpoint=API_AMBASSADOR,\n",
      "        deployment_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mseldon-model-server\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        payload_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mndarray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        namespace=\u001b[33m\"\u001b[39;49;00m\u001b[33mseldon\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mrest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    sm_result = sc.predict(data=np.array(data))\n",
      "    logging.info(sm_result)\n",
      "    result = seldon_message_to_json(sm_result.response)\n",
      "    logging.info(result)\n",
      "    values = result.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, {}).get(\u001b[33m\"\u001b[39;49;00m\u001b[33mndarray\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, {})\n",
      "    \u001b[34massert\u001b[39;49;00m (values == labels)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize integration/test_e2e_seldon_model_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now push your changes to trigger the pipeline\n",
    "Because Jenkins X has created a CI GitOps pipeline for our repo we just need to push our changes to run all the tests\n",
    "\n",
    "We can do this by running our good old git commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git add .\n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the pipeline has been triggered by viewing our activities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Create Effective Pipeline                          11h28m57s       7s Succeeded \n",
      "    Create Tekton Crds                                 11h28m50s      11s Succeeded \n",
      "  test and deploy sklearn server                       11h28m38s    1m54s Succeeded \n",
      "    Credential Initializer 59hx6                       11h28m38s       0s Succeeded \n",
      "    Working Dir Initializer Fslpm                      11h28m38s       1s Succeeded \n",
      "    Place Tools                                        11h28m37s       1s Succeeded \n",
      "    Git Source Seldonio Sig Mlops Seldon Jenki Ftjtn   11h28m36s       6s Succeeded https://github.com/SeldonIO/sig-mlops-seldon-jenkins-x.git\n",
      "    Git Merge                                          11h28m30s       1s Succeeded \n",
      "    Run Tests                                          11h28m29s      13s Succeeded \n",
      "    Build And Push Images                              11h28m16s    1m32s Succeeded \n"
     ]
    }
   ],
   "source": [
    "!jx get activity -f sig-mlops-seldon-jenkins-x | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can actually see the logs of our running job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: Failed to parse docker reference ELDON_BASE_WRAPPER\n",
      "ERROR: An error occurred: unable to get metadata for ELDON_BASE_WRAPPER:latest\n",
      "ERROR: Suggested solution: check image name\n",
      "ERROR: If the problem persists consult the docs at https://github.com/openshift/source-to-image/tree/master/docs. Eventually reach us on freenode #openshift or file an issue at https://github.com/openshift/source-to-image/issues providing us with a log from your build using log output level 3.\n",
      "Makefile:8: recipe for target 'build' failed\n",
      "make: *** [build] Error 1\n",
      "Stopping Docker: dockerProgram process in pidfile '/var/run/docker-ssd.pid', 1 process(es), refused to die.\n",
      "\u001b[31m\n",
      "Pipeline failed on stage 'test-and-deploy-sklearn-server' : container 'step-build-and-push-images'. The execution of the pipeline has stopped.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wrote: /tmp/086bfe4e-d4ac-46e6-baa1-71d4ef7abca4095596018\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "YOUR_GIT_USERNAME=SeldonIO\n",
    "jx get build logs \"$YOUR_GIT_USERNAME/sig-mlops-seldon-jenkins-x/master #7 release\" | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing your Jenkins X Application\n",
    "\n",
    "Now that we've deployed our MLOps repo, Jenkins X now has created an application from our charts.\n",
    "\n",
    "This application gets automatically syncd into the Jenkins X staging environment, which you can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -n jx-staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your application in the staging environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ndarray {\n",
       "  values {\n",
       "    number_value: 2.0\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "import numpy as np\n",
    "\n",
    "url = !kubectl get svc ambassador -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    gateway_endpoint=\"localhost:80\",\n",
    "    deployment_name=\"mlops-server\",\n",
    "    payload_type=\"ndarray\",\n",
    "    namespace=\"jx-staging\",\n",
    "    transport=\"rest\")\n",
    "\n",
    "response = sc.predict(data=np.array([twenty_test.data[0]]))\n",
    "\n",
    "response.response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"meta\": {\n",
      "    \"puid\": \"so6n21pkf70fm66eka28lc63cr\",\n",
      "    \"tags\": {\n",
      "    },\n",
      "    \"routing\": {\n",
      "    },\n",
      "    \"requestPath\": {\n",
      "      \"news-classifier-server-processor\": \"axsauze/sklearn-server:0.1\"\n",
      "    },\n",
      "    \"metrics\": []\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"names\": [],\n",
      "    \"ndarray\": [2.0]\n",
      "  }\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   350  100   278  100    72   7942   2057 --:--:-- --:--:-- --:--:-- 10294\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "     -d \"{'data': {'names': ['text'], 'ndarray': ['Hello world this is a test']}}\" \\\n",
    "    http://localhost/seldon/jx-staging/news-classifier-server/api/v0.1/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving into our continuous integration\n",
    "\n",
    "We have now separated our model development into two chunks: \n",
    "\n",
    "* The first one involves the creation of a model serve, and the second one involves the CI of the model server, and the second involves the deployment of models that create the model.\n",
    "\n",
    "\n",
    "## Using the Jenkins X pipeline\n",
    "\n",
    "In order to do this we will be able to first run some tests and the push to the docker repo.\n",
    "\n",
    "For this we will be leveraging the Jenkins X file, we'll first start with a simple file that just runs the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jenkins-x.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile jenkins-x.yml\n",
    "buildPack: none\n",
    "pipelineConfig:\n",
    "  pipelines:\n",
    "    release:\n",
    "      pipeline:\n",
    "        agent:\n",
    "          image: seldonio/core-builder:0.4\n",
    "        stages:\n",
    "          - name: test-sklearn-server\n",
    "            steps:\n",
    "            - name: run-tests\n",
    "              command: make\n",
    "              args:\n",
    "              - install_dev\n",
    "              - test\n",
    "    pullRequest:\n",
    "      pipeline:\n",
    "        agent:\n",
    "          image: seldonio/core-builder:0.4\n",
    "        stages:\n",
    "          - name: test-sklearn-server\n",
    "            steps:\n",
    "            - name: run-tests\n",
    "              command: make\n",
    "              args:\n",
    "              - install_dev\n",
    "              - test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `jenkins-x.yml` file is pretty easy to understand if we read through the different steps.\n",
    "\n",
    "Basically we can define the steps of what happens upon `release` - i.e. when a PR / Commit is added to master - and what happens upon `pullRequest` - whenever someone opens a pull request.\n",
    "\n",
    "You can see that the steps are exactly the same for both release and PR for now - namely, we run `make install_dev test` which basically installs all the dependencies and runs all the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model that we want to be able to deploy, we want to make sure that we run end-to-end tests on that model to make sure everything works as expected.\n",
    "\n",
    "For this we will leverage the same framework that the Kubernetes team uses to test Kubernetes itself: KIND.\n",
    "\n",
    "KIND stands for Kubernetes in Docker, and is used to isolate a Kubernetes environent for end-to-end tests.\n",
    "\n",
    "In our case, we will be able to leverage to create an isolated environment, where we'll be able to test our model.\n",
    "\n",
    "For this, the steps we'll have to carry out include:\n",
    "\n",
    "1. Authenticate your docker with the jx CLI\n",
    "2. Add the steps in the `Jenkins-X.yml` to run this in the production cluster\n",
    "3. Leverage the `kind_run_all.sh` script that creates a KIND cluster and runs the tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add docker auth to your cluster\n",
    "\n",
    "Adding a docker authentication with Jenkins X can be done through a JX CLI command, which is the following:\n",
    "\n",
    "* `jx create docker auth --host https://index.docker.io/v1/ --user $YOUR_DOCKER_USERNAME --secret $YOUR_DOCKER_KEY_SECRET --email $YOUR_DOCKER_EMAIL`\n",
    "\n",
    "This comamnd will use these credentials to authenticate with Docker and create an auth token (which expires)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend JenkinsX file for integration\n",
    "\n",
    "Now that we have the test that would run for the integration tests, we need to extend the JX pipeline to run this.\n",
    "\n",
    "This extension is quite simple, and only requires adding the following line:\n",
    "    \n",
    "```\n",
    "            - name: run-end-to-end-tests\n",
    "              command: bash\n",
    "              args:\n",
    "              - integration/kind_test_all.sh\n",
    "```\n",
    "\n",
    "This line would be added in both the PR and release pipelines so that we can run integration tests then.\n",
    "\n",
    "It is also possible to move the integration tests into a separate jenkins-x file such as `jenkins-x-integration.yml` by leveraging [Contexts & Schedules]() which basically allow us to extend the functionality of Prow by writing our own triggers, however this is outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config to provide docker authentication\n",
    "\n",
    "This piece is slightly more extensive, as we will need to use Docker to build out containers due to the dependency on `s2i` to build the model wrappers.\n",
    "\n",
    "First we need to define the volumes that we'll be mounting to the container.\n",
    "\n",
    "The first few volumes before basically consist of the core components that docker will need to be able to run.\n",
    "```\n",
    "          volumes:\n",
    "            - name: modules\n",
    "              hostPath:\n",
    "                path: /lib/modules\n",
    "                type: Directory\n",
    "            - name: cgroup\n",
    "              hostPath:\n",
    "                path: /sys/fs/cgroup\n",
    "                type: Directory\n",
    "            - name: dind-storage\n",
    "              emptyDir: {}\n",
    "```\n",
    "We also want to mount the docker credentials which we will generate in the next step.\n",
    "```\n",
    "            - name: jenkins-docker-config-volume\n",
    "              secret:\n",
    "                items:\n",
    "                - key: config.json\n",
    "                  path: config.json\n",
    "                secretName: jenkins-docker-cfg\n",
    "```\n",
    "Once we've created the volumes, now we just need to mount them. This can be done as follows:\n",
    "```\n",
    "        options:\n",
    "          containerOptions:\n",
    "            volumeMounts:\n",
    "              - mountPath: /lib/modules\n",
    "                name: modules\n",
    "                readOnly: true\n",
    "              - mountPath: /sys/fs/cgroup\n",
    "                name: cgroup\n",
    "              - name: dind-storage\n",
    "                mountPath: /var/lib/docker                 \n",
    "```\n",
    "And finally we also mount the docker auth configuration so we don't have to run `docker login`:\n",
    "```\n",
    "              - mountPath: /builder/home/.docker\n",
    "                name: jenkins-docker-config-volume\n",
    "```\n",
    "\n",
    "And to finalise, we need to make sure that the pod can run with privileged context.\n",
    "\n",
    "The reason why this is required is in order to be able to run the docker daemon:\n",
    "```\n",
    "            securityContext:\n",
    "              privileged: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kind run all integration tests script\n",
    "\n",
    "The kind_run_all may seem complicated at first, but it's actually quite simple. \n",
    "\n",
    "All the script does is set-up a kind cluster with all dependencies, deploy the model and clean everything up.\n",
    "\n",
    "Let's break down each of the components within the script.\n",
    "\n",
    "#### Start docker\n",
    "\n",
    "We first start the docker daemon and wait until Docker is running (using `docker ps q` for guidance.\n",
    "\n",
    "```\n",
    "# FIRST WE START THE DOCKER DAEMON\n",
    "service docker start\n",
    "# the service can be started but the docker socket not ready, wait for ready\n",
    "WAIT_N=0\n",
    "while true; do\n",
    "    # docker ps -q should only work if the daemon is ready\n",
    "    docker ps -q > /dev/null 2>&1 && break\n",
    "    if [[ ${WAIT_N} -lt 5 ]]; then\n",
    "        WAIT_N=$((WAIT_N+1))\n",
    "        echo \"[SETUP] Waiting for Docker to be ready, sleeping for ${WAIT_N} seconds ...\"\n",
    "        sleep ${WAIT_N}\n",
    "    else\n",
    "        echo \"[SETUP] Reached maximum attempts, not waiting any longer ...\"\n",
    "        break\n",
    "    fi\n",
    "done\n",
    "```\n",
    "\n",
    "#### Create and set-up KIND cluster\n",
    "\n",
    "Once we're running a docker daemon, we can run the command to create our KIND cluster, and install all the components.\n",
    "\n",
    "This will set up a Kubnernetes cluster using the docker daemon (using containers as Nodes), and then install Ambassador + Seldon Core.\n",
    "\n",
    "```\n",
    "#######################################\n",
    "# AVOID EXIT ON ERROR FOR FOLLOWING CMDS\n",
    "set +o errexit\n",
    "\n",
    "# START CLUSTER \n",
    "make kind_create_cluster\n",
    "KIND_EXIT_VALUE=$?\n",
    "\n",
    "# Ensure we reach the kubeconfig path\n",
    "export KUBECONFIG=$(kind get kubeconfig-path)\n",
    "\n",
    "# ONLY RUN THE FOLLOWING IF SUCCESS\n",
    "if [[ ${KIND_EXIT_VALUE} -eq 0 ]]; then\n",
    "    # KIND CLUSTER SETUP\n",
    "    make kind_setup\n",
    "    SETUP_EXIT_VALUE=$?\n",
    "```\n",
    "\n",
    "#### Run python tests\n",
    "\n",
    "We can now run the tests; for this we run all the dev installations and kick off our tests (which we'll add inside of the integration folder).\n",
    "\n",
    "```\n",
    "    # BUILD S2I BASE IMAGES\n",
    "    make build\n",
    "    S2I_EXIT_VALUE=$?\n",
    "\n",
    "    ## INSTALL ALL REQUIRED DEPENDENCIES\n",
    "    make install_integration_dev\n",
    "    INSTALL_EXIT_VALUE=$?\n",
    "    \n",
    "    ## RUNNING TESTS AND CAPTURING ERROR\n",
    "    make test\n",
    "    TEST_EXIT_VALUE=$?\n",
    "fi\n",
    "```\n",
    "\n",
    "#### Clean up\n",
    "\n",
    "Finally we just clean everything, including the cluster, the containers and the docker daemon.\n",
    "\n",
    "```\n",
    "# DELETE KIND CLUSTER\n",
    "make kind_delete_cluster\n",
    "DELETE_EXIT_VALUE=$?\n",
    "\n",
    "#######################################\n",
    "# EXIT STOPS COMMANDS FROM HERE ONWARDS\n",
    "set -o errexit\n",
    "\n",
    "# CLEANING DOCKER\n",
    "docker ps -aq | xargs -r docker rm -f || true\n",
    "service docker stop || true\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promote your application\n",
    "Now that we've verified that our CI pipeline is working, we want to promote our application to production\n",
    "\n",
    "This can be done with our JX CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jx promote application --..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your production application\n",
    "\n",
    "Once your production application is deployed, you can test it using the same script, but in the `jx-production` namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "import numpy as np\n",
    "\n",
    "url = !kubectl get svc ambassador -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    gateway_endpoint=\"localhost:80\",\n",
    "    deployment_name=\"mlops-server\",\n",
    "    payload_type=\"ndarray\",\n",
    "    namespace=\"jx-production\",\n",
    "    transport=\"rest\")\n",
    "\n",
    "response = sc.predict(data=np.array([twenty_test.data[0]]))\n",
    "\n",
    "response.response.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sig-mlops",
   "language": "python",
   "name": "sig-mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
